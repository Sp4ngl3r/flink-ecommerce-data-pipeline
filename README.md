# Flink Ecommerce Data Pipeline

A complete real-time ecommerce transaction data pipeline using Apache Kafka for streaming and Apache Flink for stream processing. This project simulates realistic ecommerce transactions and processes them through a modern data streaming architecture with multiple storage and analytics backends.

## ğŸ—ï¸ Architecture

This project implements a complete data streaming infrastructure with dual-language components:

- **Python Data Generator**: Generates realistic ecommerce transactions using Faker
- **Java Flink Processor**: Real-time stream processing and data aggregation
- **Apache Kafka**: Message streaming platform for real-time data
- **PostgreSQL**: Database for persistent structured storage
- **Elasticsearch**: Search and analytics engine
- **Kibana**: Data visualization dashboard

## ğŸ“ Project Structure

```
flink-ecommerce/
â”œâ”€â”€ ğŸ“ data-generator/           # Python Kafka data producer
â”‚   â”œâ”€â”€ main.py                  # Transaction generator script
â”‚   â”œâ”€â”€ requirements.txt         # Python dependencies
â”‚   â””â”€â”€ venv/                    # Python virtual environment
â”œâ”€â”€ ğŸ“ flink-ecommerce-module/   # Java Flink processing application
â”‚   â”œâ”€â”€ pom.xml                  # Maven project configuration
â”‚   â”œâ”€â”€ ğŸ“ src/main/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ java/dev/spangler/
â”‚   â”‚   â”‚   â”œâ”€â”€ DataStreamJob.java           # Main Flink application
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ dto/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Transaction.java         # Main transaction model
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SalesPerDay.java         # Daily sales aggregation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SalesPerMonth.java       # Monthly sales aggregation
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ SalesPerCategory.java    # Category sales aggregation
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“ deserializer/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ JsonKeyValueDeserializationSchema.java  # Kafka JSON deserializer
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“ utils/
â”‚   â”‚   â”‚       â””â”€â”€ JsonUtil.java            # JSON conversion utilities
â”‚   â”‚   â””â”€â”€ ğŸ“ resources/
â”‚   â”‚       â””â”€â”€ log4j2.properties           # Logging configuration
â”‚   â””â”€â”€ ğŸ“ target/               # Maven build output
â”œâ”€â”€ docker-compose.yaml          # Infrastructure services
â”œâ”€â”€ README.md                    # This documentation
â””â”€â”€ .gitignore                   # Git ignore rules
```

## ğŸ”„ Data Flow Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python Data       â”‚â”€â”€â”€â–¶â”‚   Kafka Topic:           â”‚â”€â”€â”€â–¶â”‚   Flink             â”‚
â”‚   Generator         â”‚    â”‚   financial_transactions â”‚    â”‚   DataStreamJob     â”‚
â”‚   (Faker + Kafka)   â”‚    â”‚   (Message Queue)        â”‚    â”‚   (Stream Processor)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                       â”‚
                                                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kibana            â”‚â—€â”€â”€â”€â”‚   Elasticsearch          â”‚â—€â”€â”€â”€â”‚   Processed Data    â”‚
â”‚   Dashboard         â”‚    â”‚   Index                  â”‚    â”‚   (Aggregations)    â”‚
â”‚   (Visualization)   â”‚    â”‚   (Search & Analytics)   â”‚    â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                       â”‚
                                                                       â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   PostgreSQL             â”‚â—€â”€â”€â”€â”‚   Raw Transactions  â”‚
                           â”‚   Database               â”‚    â”‚   (Structured Data) â”‚
                           â”‚   (Persistent Storage)   â”‚    â”‚                     â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flow Description:**
1. **Python Generator** â†’ Creates realistic ecommerce transactions using Faker
2. **Kafka Topic** â†’ Streams transaction data in real-time
3. **Flink Processor** â†’ Consumes, processes, and aggregates transaction data
4. **Dual Storage** â†’ Data flows to both PostgreSQL (structured) and Elasticsearch (analytics)
5. **Kibana** â†’ Provides real-time dashboards and visualizations

## ğŸ“Š Generated Data Schema

Each transaction includes:
- `transactionId`: Unique transaction identifier (UUID)
- `productId`, `productName`, `productCategory`: Product information
- `productPrice`, `productQuantity`, `totalAmount`: Pricing details
- `productBrand`: Product brand
- `currency`: Transaction currency (USD, EUR, GBP, etc.)
- `customerId`: Customer identifier
- `transactionDate`: ISO timestamp
- `paymentMethod`, `paymentStatus`: Payment information

## ğŸš€ Quick Start

### 1. Start Infrastructure Services

```bash
# Start Kafka, Zookeeper, PostgreSQL, Elasticsearch, and Kibana
docker-compose up -d

# Verify services are running
docker-compose ps
```

### 2. Set Up Python Data Generator

```bash
# Navigate to data generator
cd data-generator

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install Python dependencies
pip install -r requirements.txt
```

### 3. Set Up Java Flink Module

```bash
# Navigate to Flink module
cd flink-ecommerce-module

# Build the project with Maven
mvn clean install
```

### 4. Generate and Process Transaction Data

```bash
# Terminal 1: Start data generation
cd data-generator
python main.py

# Terminal 2: Run Flink processing job
cd flink-ecommerce-module
~path/to/flink run -c dev.spangler.DataStreamJob target/flink-ecommerce-module-1.0-SNAPSHOT.jar
```

## ğŸ”§ Services and Ports

| Service       | Port | Purpose | Access URL |
|---------------|------|---------|------------|
| Kafka Broker  | 9092 | Message streaming | localhost:9092 |
| Zookeeper     | 2181 | Kafka coordination | localhost:2181 |
| PostgreSQL    | 5433 | Data storage | localhost:5433 |
| Elasticsearch | 9200 | Search and analytics | http://localhost:9200 |
| Kibana        | 5601 | Data visualization | http://localhost:5601 |

## ğŸ’» Core Components

### Python Data Generator (`data-generator/`)
- **Framework**: Uses `confluent-kafka` for Kafka integration
- **Data Generation**: Uses `faker` library for realistic transaction data
- **Output**: Produces to `financial_transactions` Kafka topic
- **Frequency**: Generates transactions every 5 seconds for 2 minutes

### Java Flink Processor (`flink-ecommerce-module/`)
- **Framework**: Apache Flink 1.20.0 with Maven
- **Key Dependencies**:
  - `flink-connector-kafka` for Kafka integration
  - `postgresql` driver for database connectivity
  - `jackson-databind` for JSON processing
  - `flink-connector-elasticsearch7` for Elasticsearch integration
  - `flink-connector-jdbc` for database operations

### Data Models
- **Transaction DTO**: Complete ecommerce transaction structure
- **Aggregation Models**: `SalesPerDay`, `SalesPerMonth`, `SalesPerCategory`
- **Serialization**: Custom JSON deserializer for Kafka message processing

## ğŸ› ï¸ Development

### Monitor Kafka Topics

```bash
# List topics
docker exec -it broker kafka-topics --bootstrap-server localhost:9092 --list

# Consume messages
docker exec -it broker kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic financial_transactions \
  --from-beginning
```

### Database Access

```bash
# Connect to PostgreSQL
docker exec -it postgres psql -U postgres -d postgres

# View transaction tables
\dt
SELECT * FROM transactions LIMIT 10;
```

### Elasticsearch Queries

```bash
# Check Elasticsearch indices
curl http://localhost:9200/_cat/indices

# Query transaction data
curl -X GET "localhost:9200/transactions/_search?pretty"
```

## ğŸ“ˆ Analytics and Visualization

### Kibana Dashboard Setup
1. Access Kibana at http://localhost:5601
2. Create index patterns for transaction data
3. Build visualizations for:
   - Sales by category
   - Transaction volume over time
   - Payment method distribution
   - Geographic sales analysis

### Real-time Metrics
The Flink job processes and aggregates:
- **Daily Sales**: Total sales amount per day
- **Monthly Sales**: Monthly sales aggregations
- **Category Performance**: Sales performance by product category
- **Real-time Alerts**: Failed transaction monitoring

## ğŸ” Troubleshooting

### Common Issues

1. **Kafka Connection Errors**: Ensure Docker services are running
   ```bash
   docker-compose ps
   docker-compose logs broker
   ```

2. **Maven Build Issues**: Ensure Java 11+ is installed
   ```bash
   java -version
   mvn -version
   ```

3. **Port Conflicts**: Check if ports 9092, 5433, 9200, 5601 are available
   ```bash
   lsof -i :9092
   ```

4. **Python Dependencies**: Ensure virtual environment is activated
   ```bash
   which python
   pip list
   ```

### Health Checks

```bash
# Check all services
docker-compose ps

# View specific service logs
docker-compose logs kafka
docker-compose logs postgres
docker-compose logs elasticsearch
```

## ğŸš€ Next Steps

This pipeline provides the foundation for:
- **Real-time Analytics**: Stream processing with Apache Flink
- **Customer Behavior Analysis**: Transaction pattern recognition
- **Fraud Detection Systems**: Anomaly detection in real-time
- **Inventory Management**: Product performance tracking
- **Business Intelligence**: Comprehensive sales dashboards
- **Machine Learning**: Feature engineering for ML models

## ğŸ”„ Extending the System

### Adding New Features

1. **Additional Data Sources**: Extend the Python generator with customer demographics
2. **Complex Aggregations**: Add window-based analytics in Flink
3. **Alert Systems**: Implement real-time alerting for business rules
4. **Data Quality**: Add data validation and cleansing steps
5. **Scalability**: Deploy on Kubernetes for production workloads

### Technology Integration

- **Apache Airflow**: For batch processing and orchestration
- **Apache Superset**: For advanced business intelligence
- **Redis**: For caching and session management
- **Docker Swarm/Kubernetes**: For container orchestration

## ğŸ“„ License

This project is open source and available under the MIT License. 